{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442d3b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.7.0-cp38-cp38-macosx_10_11_x86_64.whl (207.1 MB)\n",
      "Collecting gast<0.5.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.20.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.22.0-cp38-cp38-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-12.0.0-py2.py3-none-macosx_10_9_x86_64.whl (12.2 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.19.1-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.42.0-cp38-cp38-macosx_10_10_x86_64.whl (4.0 MB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/anirrudhramesh/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 importlib-metadata-4.8.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.8 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd6df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time\n",
    "from spinup.algos.tf1.ddpg import core\n",
    "from spinup.algos.tf1.ddpg.core import get_vars\n",
    "from spinup.utils.logx import EpochLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f0920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for DDPG agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(obs1=self.obs1_buf[idxs],\n",
    "                    obs2=self.obs2_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])\n",
    "\n",
    "\n",
    "\n",
    "def ddpg(env_fn, actor_critic=core.mlp_actor_critic, ac_kwargs=dict(), seed=0, \n",
    "         steps_per_epoch=4000, epochs=100, replay_size=int(1e6), gamma=0.99, \n",
    "         polyak=0.995, pi_lr=1e-3, q_lr=1e-3, batch_size=100, start_steps=10000, \n",
    "         update_after=1000, update_every=50, act_noise=0.1, num_test_episodes=10, \n",
    "         max_ep_len=1000, logger_kwargs=dict(), save_freq=1):\n",
    "    \"\"\"\n",
    "    Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "\n",
    "        actor_critic: A function which takes in placeholder symbols \n",
    "            for state, ``x_ph``, and action, ``a_ph``, and returns the main \n",
    "            outputs from the agent's Tensorflow computation graph:\n",
    "\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``pi``       (batch, act_dim)  | Deterministically computes actions\n",
    "                                           | from policy given states.\n",
    "            ``q``        (batch,)          | Gives the current estimate of Q* for \n",
    "                                           | states in ``x_ph`` and actions in\n",
    "                                           | ``a_ph``.\n",
    "            ``q_pi``     (batch,)          | Gives the composition of ``q`` and \n",
    "                                           | ``pi`` for states in ``x_ph``: \n",
    "                                           | q(x, pi(x)).\n",
    "            ===========  ================  ======================================\n",
    "\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the actor_critic \n",
    "            function you provided to DDPG.\n",
    "\n",
    "        seed (int): Seed for random number generators.\n",
    "\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "\n",
    "        epochs (int): Number of epochs to run and train agent.\n",
    "\n",
    "        replay_size (int): Maximum length of replay buffer.\n",
    "\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "\n",
    "        polyak (float): Interpolation factor in polyak averaging for target \n",
    "            networks. Target networks are updated towards main networks \n",
    "            according to:\n",
    "\n",
    "            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow \n",
    "                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n",
    "\n",
    "            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually \n",
    "            close to 1.)\n",
    "\n",
    "        pi_lr (float): Learning rate for policy.\n",
    "\n",
    "        q_lr (float): Learning rate for Q-networks.\n",
    "\n",
    "        batch_size (int): Minibatch size for SGD.\n",
    "\n",
    "        start_steps (int): Number of steps for uniform-random action selection,\n",
    "            before running real policy. Helps exploration.\n",
    "\n",
    "        update_after (int): Number of env interactions to collect before\n",
    "            starting to do gradient descent updates. Ensures replay buffer\n",
    "            is full enough for useful updates.\n",
    "\n",
    "        update_every (int): Number of env interactions that should elapse\n",
    "            between gradient descent updates. Note: Regardless of how long \n",
    "            you wait between updates, the ratio of env steps to gradient steps \n",
    "            is locked to 1.\n",
    "\n",
    "        act_noise (float): Stddev for Gaussian exploration noise added to \n",
    "            policy at training time. (At test time, no noise is added.)\n",
    "\n",
    "        num_test_episodes (int): Number of episodes to test the deterministic\n",
    "            policy at the end of each epoch.\n",
    "\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env, test_env = env_fn(), env_fn()\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "    act_limit = env.action_space.high[0]\n",
    "\n",
    "    # Share information about action space with policy architecture\n",
    "    ac_kwargs['action_space'] = env.action_space\n",
    "\n",
    "    # Inputs to computation graph\n",
    "    x_ph, a_ph, x2_ph, r_ph, d_ph = core.placeholders(obs_dim, act_dim, obs_dim, None, None)\n",
    "\n",
    "    # Main outputs from computation graph\n",
    "    with tf.variable_scope('main'):\n",
    "        pi, q, q_pi = actor_critic(x_ph, a_ph, **ac_kwargs)\n",
    "    \n",
    "    # Target networks\n",
    "    with tf.variable_scope('target'):\n",
    "        # Note that the action placeholder going to actor_critic here is \n",
    "        # irrelevant, because we only need q_targ(s, pi_targ(s)).\n",
    "        pi_targ, _, q_pi_targ  = actor_critic(x2_ph, a_ph, **ac_kwargs)\n",
    "\n",
    "    # Experience buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "    # Count variables\n",
    "    var_counts = tuple(core.count_vars(scope) for scope in ['main/pi', 'main/q', 'main'])\n",
    "    print('\\nNumber of parameters: \\t pi: %d, \\t q: %d, \\t total: %d\\n'%var_counts)\n",
    "\n",
    "    # Bellman backup for Q function\n",
    "    backup = tf.stop_gradient(r_ph + gamma*(1-d_ph)*q_pi_targ)\n",
    "\n",
    "    # DDPG losses\n",
    "    pi_loss = -tf.reduce_mean(q_pi)\n",
    "    q_loss = tf.reduce_mean((q-backup)**2)\n",
    "\n",
    "    # Separate train ops for pi, q\n",
    "    pi_optimizer = tf.train.AdamOptimizer(learning_rate=pi_lr)\n",
    "    q_optimizer = tf.train.AdamOptimizer(learning_rate=q_lr)\n",
    "    train_pi_op = pi_optimizer.minimize(pi_loss, var_list=get_vars('main/pi'))\n",
    "    train_q_op = q_optimizer.minimize(q_loss, var_list=get_vars('main/q'))\n",
    "\n",
    "    # Polyak averaging for target variables\n",
    "    target_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_main)\n",
    "                              for v_main, v_targ in zip(get_vars('main'), get_vars('target'))])\n",
    "\n",
    "    # Initializing targets to match main variables\n",
    "    target_init = tf.group([tf.assign(v_targ, v_main)\n",
    "                              for v_main, v_targ in zip(get_vars('main'), get_vars('target'))])\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(target_init)\n",
    "\n",
    "    # Setup model saving\n",
    "    logger.setup_tf_saver(sess, inputs={'x': x_ph, 'a': a_ph}, outputs={'pi': pi, 'q': q})\n",
    "\n",
    "    def get_action(o, noise_scale):\n",
    "        a = sess.run(pi, feed_dict={x_ph: o.reshape(1,-1)})[0]\n",
    "        a += noise_scale * np.random.randn(act_dim)\n",
    "        return np.clip(a, -act_limit, act_limit)\n",
    "\n",
    "    def test_agent():\n",
    "        for j in range(num_test_episodes):\n",
    "            o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "            while not(d or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time (noise_scale=0)\n",
    "                o, r, d, _ = test_env.step(get_action(o, 0))\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "            logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for t in range(total_steps):\n",
    "\n",
    "        # Until start_steps have elapsed, randomly sample actions\n",
    "        # from a uniform distribution for better exploration. Afterwards, \n",
    "        # use the learned policy (with some noise, via act_noise). \n",
    "        if t > start_steps:\n",
    "            a = get_action(o, act_noise)\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len==max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "        # Super critical, easy to overlook step: make sure to update \n",
    "        # most recent observation!\n",
    "        o = o2\n",
    "\n",
    "        # End of trajectory handling\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "        # Update handling\n",
    "        if t >= update_after and t % update_every == 0:\n",
    "            for _ in range(update_every):\n",
    "                batch = replay_buffer.sample_batch(batch_size)\n",
    "                feed_dict = {x_ph: batch['obs1'],\n",
    "                             x2_ph: batch['obs2'],\n",
    "                             a_ph: batch['acts'],\n",
    "                             r_ph: batch['rews'],\n",
    "                             d_ph: batch['done']\n",
    "                            }\n",
    "\n",
    "                # Q-learning update\n",
    "                outs = sess.run([q_loss, q, train_q_op], feed_dict)\n",
    "                logger.store(LossQ=outs[0], QVals=outs[1])\n",
    "\n",
    "                # Policy update\n",
    "                outs = sess.run([pi_loss, train_pi_op, target_update], feed_dict)\n",
    "                logger.store(LossPi=outs[0])\n",
    "\n",
    "        # End of epoch wrap-up\n",
    "        if (t+1) % steps_per_epoch == 0:\n",
    "            epoch = (t+1) // steps_per_epoch\n",
    "\n",
    "            # Save model\n",
    "            if (epoch % save_freq == 0) or (epoch == epochs):\n",
    "                logger.save_state({'env': env}, None)\n",
    "\n",
    "            # Test the performance of the deterministic version of the agent.\n",
    "            test_agent()\n",
    "\n",
    "            # Log info about epoch\n",
    "            logger.log_tabular('Epoch', epoch)\n",
    "            logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "            logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
    "            logger.log_tabular('EpLen', average_only=True)\n",
    "            logger.log_tabular('TestEpLen', average_only=True)\n",
    "            logger.log_tabular('TotalEnvInteracts', t)\n",
    "            logger.log_tabular('QVals', with_min_and_max=True)\n",
    "            logger.log_tabular('LossPi', average_only=True)\n",
    "            logger.log_tabular('LossQ', average_only=True)\n",
    "            logger.log_tabular('Time', time.time()-start_time)\n",
    "            logger.dump_tabular()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env', type=str, default='HalfCheetah-v2')\n",
    "    parser.add_argument('--hid', type=int, default=256)\n",
    "    parser.add_argument('--l', type=int, default=2)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--seed', '-s', type=int, default=0)\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--exp_name', type=str, default='ddpg')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    from spinup.utils.run_utils import setup_logger_kwargs\n",
    "    logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed)\n",
    "\n",
    "    ddpg(lambda : gym.make(args.env), actor_critic=core.mlp_actor_critic,\n",
    "         ac_kwargs=dict(hidden_sizes=[args.hid]*args.l),\n",
    "         gamma=args.gamma, seed=args.seed, epochs=args.epochs,\n",
    "         logger_kwargs=logger_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede5ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
